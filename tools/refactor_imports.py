#!/usr/bin/env python3
"""CLI tool to normalize imports for mixed src-layout repositories."""
from __future__ import annotations

import csv
import hashlib
import io
import json
import logging
import os
import re
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Callable, Dict, Iterable, List, Optional, Sequence, Tuple
import unicodedata
import uuid

import math

import libcst as cst
from dateutil.tz import gettz
from prometheus_client import CollectorRegistry, Counter, Gauge, generate_latest
from tenacity import RetryError, Retrying, retry_if_exception_type, stop_after_attempt
from tenacity.wait import wait_base
import typer

try:
    import orjson
except ModuleNotFoundError:  # pragma: no cover - fallback path
    orjson = None  # type: ignore

import pathspec

# ---------------------------------------------------------------------------
# Constants & deterministic helpers
# ---------------------------------------------------------------------------

APP = typer.Typer(help="Refactor import statements for src-layout compliance.")

AGENTS_FILENAME = "AGENTS.md"
SRC_PREFIX = "src"
ABS_PREFIX = f"{SRC_PREFIX}."
DEFAULT_CLOCK = "1403-01-01T00:00:00+03:30"
TZ_ASIA_TEHRAN = gettz("Asia/Tehran")
CSV_HEADER = ["file", "action", "original", "updated"]
AUTO_INIT_COMMENT = "# Generated by tools.refactor_imports to enable package discovery\n"
MAX_RATE_LIMIT = 300
IDEMPOTENCY_TTL_SECONDS = 24 * 60 * 60
PERSIAN_DIGITS = str.maketrans("۰۱۲۳۴۵۶۷۸۹", "0123456789")
ARABIC_DIGITS = str.maketrans("٠١٢٣٤٥٦٧٨٩", "0123456789")
CONTROL_STRIP = {ch for ch in map(chr, range(32)) if ch not in {"\t", "\n", "\r"}}

LOGGER = logging.getLogger("refactor_imports")
LOGGER.setLevel(logging.INFO)


class RefactorError(Exception):
    """Deterministic Persian error for CLI exits."""

    def __init__(self, message: str) -> None:
        super().__init__(message)
        self.message = message

    def to_exit(self) -> "typer.Exit":
        typer.echo(self.message)
        return typer.Exit(code=1)

# ---------------------------------------------------------------------------
# Structured logging helpers
# ---------------------------------------------------------------------------


def _default_json_dumps(payload: Dict[str, object]) -> str:
    if orjson is not None:  # pragma: no branch
        return orjson.dumps(payload).decode("utf-8")
    return json.dumps(payload, ensure_ascii=False, sort_keys=True)


class StructuredLogHandler(logging.Handler):
    """Emit structured JSON logs with deterministic fields."""

    def __init__(self, correlation_id: str, namespace: str) -> None:
        super().__init__()
        self._correlation_id = correlation_id
        self._namespace = namespace

    def emit(self, record: logging.LogRecord) -> None:  # pragma: no cover - logging path
        payload: Dict[str, object] = {
            "correlation_id": self._correlation_id,
            "namespace": self._namespace,
            "level": record.levelname,
            "message": record.getMessage(),
        }
        if hasattr(record, "extra_data"):
            payload.update(record.extra_data)  # type: ignore[arg-type]
        stream = _default_json_dumps(payload)
        stream = stream.replace("\n", " ").replace("\r", " ")
        sys.stdout.write(stream + "\n")


# ---------------------------------------------------------------------------
# Deterministic wait strategy for retries
# ---------------------------------------------------------------------------


class DeterministicWait(wait_base):
    """Tenacity wait strategy with deterministic exponential backoff."""

    def __init__(self, seed: str) -> None:
        self._seed = hashlib.blake2s(seed.encode("utf-8"), digest_size=8).digest()

    def __call__(self, retry_state: "RetryCallState") -> float:  # type: ignore[override]
        attempt = retry_state.attempt_number
        base_delay = 0.05 * (2 ** (attempt - 1))
        jitter_source = hashlib.blake2s(
            self._seed + attempt.to_bytes(2, "big"), digest_size=4
        ).hexdigest()
        jitter = int(jitter_source, 16) % 50 / 1000.0
        return base_delay + jitter


# ---------------------------------------------------------------------------
# Domain dataclasses
# ---------------------------------------------------------------------------


@dataclass(frozen=True)
class ModuleFix:
    file_path: Path
    action: str
    original: str
    updated: str


@dataclass
class RefactorStats:
    total_files: int = 0
    fixes: List[ModuleFix] = field(default_factory=list)
    created_init: List[Path] = field(default_factory=list)

    def add_fix(self, fix: ModuleFix) -> None:
        self.fixes.append(fix)


@dataclass
class RefactorConfig:
    convert_relative: bool = False
    apply_changes: bool = False
    fix_entrypoint: Optional[str] = None
    report_csv: Optional[Path] = None
    report_json: Optional[Path] = None
    correlation_id: Optional[str] = None
    rate_limit: int = 30
    registry: Optional[CollectorRegistry] = None
    serve_metrics: bool = False
    metrics_port: int = 9123
    metrics_token: Optional[str] = None
    metrics_requests: int = 1
    clock: Optional[Callable[[], int]] = None


@dataclass
class RepoContext:
    root: Path
    src: Path
    namespace: str
    run_id: str
    correlation_id: str


@dataclass
class ModuleGraph:
    bare_to_full: Dict[str, str]
    module_to_path: Dict[str, Path]
    package_modules: List[str]


# ---------------------------------------------------------------------------
# Middleware primitives
# ---------------------------------------------------------------------------


@dataclass
class RequestEnvelope:
    rid: str
    action: str
    identity: str = "cli"
    namespace: str = "default"
    trace: List[str] = field(default_factory=list)
    metadata: Dict[str, str] = field(default_factory=dict)
    method: str = "POST"


class Middleware:
    def __call__(self, request: RequestEnvelope, call_next: Callable[[RequestEnvelope], object]) -> object:
        raise NotImplementedError


class RateLimitMiddleware(Middleware):
    def __init__(self, limit: int) -> None:
        self._limit = max(1, min(limit, MAX_RATE_LIMIT))
        self._count = 0

    def __call__(self, request: RequestEnvelope, call_next: Callable[[RequestEnvelope], object]) -> object:
        request.trace.append("rate-limit")
        if self._count >= self._limit:
            raise RuntimeError("❌ خطای محدودیت نرخ: درخواست بیش از حد." )
        self._count += 1
        request.metadata["rate"] = str(self._count)
        return call_next(request)


class IdempotencyStore:
    def __init__(self, clock: Optional[Callable[[], int]] = None) -> None:
        self._entries: Dict[str, Tuple[str, int]] = {}
        self._clock = clock or (lambda: 0)

    def _now(self) -> int:
        return int(self._clock())

    def clear(self) -> None:
        self._entries.clear()

    def cleanup(self) -> None:
        now = self._now()
        expired = [key for key, (_, expiry) in self._entries.items() if expiry <= now]
        for key in expired:
            self._entries.pop(key, None)

    def check_and_set(self, key: str, value: str, *, method: str) -> bool:
        self.cleanup()
        now = self._now()
        stored = self._entries.get(key)
        if stored:
            stored_identity, expiry = stored
            if expiry <= now:
                self._entries.pop(key, None)
            else:
                if method.upper() == "GET":
                    return True
                return False
        self._entries[key] = (value, now + IDEMPOTENCY_TTL_SECONDS)
        return True


class IdempotencyMiddleware(Middleware):
    def __init__(self, store: IdempotencyStore, ttl_hours: int = 24) -> None:
        self._store = store
        self._ttl_hours = ttl_hours

    def __call__(self, request: RequestEnvelope, call_next: Callable[[RequestEnvelope], object]) -> object:
        request.trace.append("idempotency")
        key = f"{request.namespace}:{request.action}:{request.rid}"
        allowed = self._store.check_and_set(key, request.identity, method=request.method)
        if not allowed:
            raise RuntimeError("❌ درخواست تکراری تشخیص داده شد؛ لطفاً شناسهٔ جدید استفاده کنید.")
        request.metadata["idempotent"] = "1"
        return call_next(request)


class AuthMiddleware(Middleware):
    def __call__(self, request: RequestEnvelope, call_next: Callable[[RequestEnvelope], object]) -> object:
        request.trace.append("auth")
        if request.identity != "cli":
            raise RuntimeError("❌ هویت نامعتبر است.")
        request.metadata["auth"] = "ok"
        return call_next(request)


class MiddlewareChain:
    def __init__(self, rate: Middleware, idem: Middleware, auth: Middleware) -> None:
        self._rate = rate
        self._idem = idem
        self._auth = auth

    def execute(self, request: RequestEnvelope, handler: Callable[[RequestEnvelope], object]) -> object:
        return self._rate(request, lambda req: self._idem(req, lambda req2: self._auth(req2, handler)))


# ---------------------------------------------------------------------------
# Utility helpers
# ---------------------------------------------------------------------------


def ensure_agents_file(root: Path) -> None:
    if not (root / AGENTS_FILENAME).exists():
        raise RefactorError(
            "❌ وارد کردن AGENTS.md اجباری است: پروندهٔ AGENTS.md در ریشهٔ مخزن یافت نشد؛ لطفاً مطابق استاندارد agents.md اضافه کنید."
        )


def detect_src_path(root: Path) -> Path:
    candidate = root / SRC_PREFIX
    if not candidate.exists():
        raise RefactorError("❌ عدم رعایت الگوی src/: پوشهٔ src یافت نشد.")
    return candidate


def build_namespace(root: Path) -> str:
    return root.name.replace(" ", "-")


def build_run_id(paths: Sequence[Path], namespace: str) -> str:
    joined = "\n".join(str(p) for p in sorted(paths))
    repo_digest = hashlib.sha256(joined.encode("utf-8")).hexdigest()
    payload = f"{DEFAULT_CLOCK}|{repo_digest}|{namespace}"
    run_digest = hashlib.sha256(payload.encode("utf-8")).hexdigest()
    return run_digest[:16]


def detect_correlation_id(config: RefactorConfig, run_id: str) -> str:
    if config.correlation_id:
        return config.correlation_id
    env_value = os.getenv("X_CORRELATION_ID")
    if env_value:
        return env_value
    namespace = run_id.split("-")[0]
    deterministic_uuid = uuid.uuid5(uuid.NAMESPACE_DNS, run_id + namespace)
    return str(deterministic_uuid)


def normalize_persian_value(value: str) -> str:
    value = unicodedata.normalize("NFKC", value)
    value = value.translate(PERSIAN_DIGITS).translate(ARABIC_DIGITS)
    value = value.replace("ي", "ی").replace("ك", "ک")
    value = "".join(ch for ch in value if ch not in CONTROL_STRIP and ch not in {"\u200c", "\u200f", "\u200e"})
    return value.strip()


def _formula_guard(value: str) -> str:
    if not value:
        return value
    if value[0] in "=+-@":
        return "'" + value
    return value


def sanitize_for_log(value: str) -> str:
    digest = hashlib.blake2s(value.encode("utf-8"), digest_size=6).hexdigest()
    return f"h:{digest}"


def compute_p95(values: Sequence[float]) -> float:
    if not values:
        return 0.0
    ordered = sorted(values)
    index = max(0, math.ceil(0.95 * len(ordered)) - 1)
    return ordered[index]


def write_report_csv(path: Path, fixes: Sequence[ModuleFix]) -> None:
    rows: List[List[str]] = []
    for fix in fixes:
        normalized = [
            normalize_persian_value(str(fix.file_path)),
            normalize_persian_value(fix.action),
            normalize_persian_value(fix.original),
            normalize_persian_value(fix.updated),
        ]
        guarded = [_formula_guard(item) for item in normalized]
        rows.append(guarded)
    _atomic_write_csv(path, rows)


def write_report_json(path: Path, context: RepoContext, stats: RefactorStats) -> None:
    payload = {
        "run_id": context.run_id,
        "namespace": context.namespace,
        "fixes": [
            {
                "file": str(fix.file_path),
                "action": fix.action,
                "original": fix.original,
                "updated": fix.updated,
            }
            for fix in stats.fixes
        ],
        "created_init": [str(p) for p in stats.created_init],
    }
    data = _default_json_dumps(payload)
    atomic_write_text(path, data)


def create_metrics_registry(existing: Optional[CollectorRegistry] = None) -> CollectorRegistry:
    return existing if existing is not None else CollectorRegistry()


def _atomic_write_csv(path: Path, rows: Sequence[Sequence[str]]) -> None:
    output = io.StringIO()
    writer = csv.writer(output, quoting=csv.QUOTE_ALL, lineterminator="\r\n")
    writer.writerow([_formula_guard(normalize_persian_value(item)) for item in CSV_HEADER])
    for row in rows:
        writer.writerow(row)
    data = output.getvalue()
    if not data.endswith("\r\n"):
        data += "\r\n"
    atomic_write_text(path, data)


def atomic_write_text(path: Path, data: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path = path.with_suffix(path.suffix + ".part")
    with open(tmp_path, "w", encoding="utf-8", newline="") as handle:
        handle.write(data)
        handle.flush()
        os.fsync(handle.fileno())
    os.replace(tmp_path, path)


# ---------------------------------------------------------------------------
# Module graph & parsing utilities
# ---------------------------------------------------------------------------


def iter_python_files(root: Path) -> List[Path]:
    ignore_lines = [
        "*.pyc",
        "__pycache__/",
        ".venv/",
        "venv/",
        "dist/",
        "build/",
        "tmp/",
    ]
    gitignore = root / ".gitignore"
    if gitignore.exists():
        ignore_lines.extend(gitignore.read_text(encoding="utf-8").splitlines())
    spec = pathspec.PathSpec.from_lines("gitwildmatch", ignore_lines)
    files: List[Path] = []
    for path in root.rglob("*.py"):
        rel = path.relative_to(root)
        if spec.match_file(str(rel)):
            continue
        files.append(path)
    return files


def build_module_graph(src: Path) -> ModuleGraph:
    bare_to_full: Dict[str, str] = {}
    module_to_path: Dict[str, Path] = {}
    package_modules: List[str] = []
    for path in src.rglob("*.py"):
        if "__pycache__" in path.parts:
            continue
        rel = path.relative_to(src)
        parts = list(rel.parts)
        if parts[-1] == "__init__.py":
            module_parts = parts[:-1]
            module = ".".join([SRC_PREFIX, *module_parts]) if module_parts else SRC_PREFIX
            package_modules.append(module)
            module_to_path[module] = path
            bare = module[len(ABS_PREFIX):] if module.startswith(ABS_PREFIX) else module
            bare_to_full[bare] = module
            continue
        module_parts = parts[:-1] + [parts[-1][:-3]]
        module = ".".join([SRC_PREFIX, *module_parts])
        bare = module[len(ABS_PREFIX):]
        module_to_path[module] = path
        bare_to_full[bare] = module
        for depth in range(1, len(module_parts) + 1):
            pkg = ".".join([SRC_PREFIX, *module_parts[:depth]])
            if pkg not in package_modules:
                package_modules.append(pkg)
                bare_pkg = pkg[len(ABS_PREFIX):] if pkg.startswith(ABS_PREFIX) else pkg
                bare_to_full.setdefault(bare_pkg, pkg)
    return ModuleGraph(bare_to_full=bare_to_full, module_to_path=module_to_path, package_modules=package_modules)


def module_name_from_path(path: Path, context: RepoContext) -> str:
    def _is_relative(candidate: Path, parent: Path) -> bool:
        try:
            candidate.relative_to(parent)
            return True
        except ValueError:
            return False

    if _is_relative(path, context.src):
        rel = path.relative_to(context.src)
        parts = list(rel.parts)
        if parts[-1] == "__init__.py":
            module_parts = parts[:-1]
        else:
            module_parts = parts[:-1] + [parts[-1][:-3]]
        if not module_parts:
            return SRC_PREFIX
        return ".".join([SRC_PREFIX, *module_parts])
    rel = path.relative_to(context.root)
    parts = rel.with_suffix("").parts
    return ".".join(parts)


# ---------------------------------------------------------------------------
# CST helpers
# ---------------------------------------------------------------------------


def dotted_name_from_node(node: cst.BaseExpression) -> Optional[str]:
    if isinstance(node, cst.Name):
        return node.value
    if isinstance(node, cst.Attribute):
        left = dotted_name_from_node(node.value)
        if left is None:
            return None
        return f"{left}.{node.attr.value}"
    return None


def dotted_name_to_node(name: str) -> cst.BaseExpression:
    parts = name.split(".")
    node: cst.BaseExpression = cst.Name(parts[0])
    for part in parts[1:]:
        node = cst.Attribute(value=node, attr=cst.Name(part))
    return node


# ---------------------------------------------------------------------------
# CST Transformer
# ---------------------------------------------------------------------------


class ImportTransformer(cst.CSTTransformer):
    def __init__(
        self,
        *,
        module_name: str,
        mapping: Dict[str, str],
        convert_relative: bool,
        record_change: Callable[[str, str], None],
    ) -> None:
        self._module_name = module_name
        self._mapping = mapping
        self._convert_relative = convert_relative
        self._record_change = record_change

    def _resolve_relative(self, module: Optional[str], level: int) -> Optional[str]:
        if level <= 0:
            return module
        current_parts = self._module_name.split(".")
        if len(current_parts) <= level:
            return None
        base = current_parts[:-level]
        if module:
            base.extend(module.split("."))
        return ".".join(base)

    def leave_Import(self, original_node: cst.Import, updated_node: cst.Import) -> cst.Import:
        new_names: List[cst.ImportAlias] = []
        for alias in updated_node.names:
            dotted = dotted_name_from_node(alias.name)
            if dotted and not dotted.startswith(ABS_PREFIX):
                replacement = self._mapping.get(dotted)
                if replacement:
                    self._record_change(dotted, replacement)
                    alias = alias.with_changes(name=dotted_name_to_node(replacement))
            new_names.append(alias)
        return updated_node.with_changes(names=new_names)

    def leave_ImportFrom(self, original_node: cst.ImportFrom, updated_node: cst.ImportFrom) -> cst.ImportFrom:
        module_expr = updated_node.module
        module_name = dotted_name_from_node(module_expr) if module_expr else None
        level = updated_node.relative.count(".") if updated_node.relative else 0
        if level and self._convert_relative:
            absolute = self._resolve_relative(module_name, level)
            if absolute:
                replacement = absolute if absolute.startswith(SRC_PREFIX) else self._mapping.get(absolute)
                if replacement:
                    self._record_change(module_name or "", replacement)
                    return updated_node.with_changes(relative=None, module=dotted_name_to_node(replacement))
        if module_name and not module_name.startswith(ABS_PREFIX):
            replacement = self._mapping.get(module_name)
            if replacement:
                self._record_change(module_name, replacement)
                return updated_node.with_changes(module=dotted_name_to_node(replacement))
        return updated_node


# ---------------------------------------------------------------------------
# Core refactor implementation
# ---------------------------------------------------------------------------


class ImportRefactorer:
    def __init__(self, context: RepoContext, config: RefactorConfig, *, store: Optional[IdempotencyStore] = None) -> None:
        self.context = context
        self.config = config
        self.graph = build_module_graph(context.src)
        self.store = store or IdempotencyStore()
        self.created_inits: List[Path] = []
        self.registry = create_metrics_registry(config.registry)
        self.metric_changes = Counter(
            "refactor_changes_total",
            "Number of import changes applied",
            registry=self.registry,
        )
        self.metric_retry_exhausted = Counter(
            "retry_exhausted_total",
            "Number of retries exhausted",
            registry=self.registry,
        )
        self.metric_files = Gauge(
            "refactor_files_total",
            "Total python files processed",
            registry=self.registry,
        )

    def _log_fix(self, fix: ModuleFix) -> None:
        LOGGER.info(
            "import-rewrite",
            extra={
                "extra_data": {
                    "rid": self.context.run_id,
                    "action": fix.action,
                    "file": str(fix.file_path),
                    "status": "candidate" if not self.config.apply_changes else "applied",
                    "before": sanitize_for_log(fix.original),
                    "after": sanitize_for_log(fix.updated),
                }
            },
        )

    def process(self, files: Sequence[Path]) -> RefactorStats:
        stats = RefactorStats(total_files=len(files))
        self.metric_files.set(len(files))
        for path in files:
            fixes = self._process_file(path)
            for fix in fixes:
                stats.add_fix(fix)
                self._log_fix(fix)
                self.metric_changes.inc()
        stats.created_init.extend(self.created_inits)
        return stats

    def _process_file(self, path: Path) -> List[ModuleFix]:
        try:
            source = path.read_text(encoding="utf-8")
        except UnicodeDecodeError:
            return []
        module_name = module_name_from_path(path, self.context)
        fixes: List[ModuleFix] = []

        def record_change(original: str, updated: str) -> None:
            if original == updated:
                return
            fixes.append(
                ModuleFix(
                    file_path=path,
                    action="import_rewrite",
                    original=original,
                    updated=updated,
                )
            )

        module = cst.parse_module(source)
        transformer = ImportTransformer(
            module_name=module_name,
            mapping=self.graph.bare_to_full,
            convert_relative=self.config.convert_relative,
            record_change=record_change,
        )
        updated = module.visit(transformer)
        if not fixes:
            return []
        if self.config.apply_changes:
            self._write_with_retry(path, updated.code)
            for fix in fixes:
                self._ensure_init_files(fix.updated)
        return fixes

    def _ensure_init_files(self, module_name: str) -> None:
        if not module_name.startswith(ABS_PREFIX):
            return
        rel = module_name[len(ABS_PREFIX):]
        parts = rel.split(".")
        depth_limit = len(parts)
        # Determine if module refers to a file or package
        if module_name not in self.graph.package_modules:
            depth_limit -= 1
        for depth in range(1, depth_limit + 1):
            package_path = self.context.src.joinpath(*parts[:depth])
            init_file = package_path / "__init__.py"
            if not package_path.exists():
                continue
            if init_file.exists():
                continue
            atomic_write_text(init_file, AUTO_INIT_COMMENT)
            if init_file not in self.created_inits:
                self.created_inits.append(init_file)

    def _write_with_retry(self, path: Path, data: str) -> None:
        retryer = Retrying(
            stop=stop_after_attempt(3),
            wait=DeterministicWait(str(path)),
            retry=retry_if_exception_type(OSError),
            reraise=True,
        )
        try:
            for attempt in retryer:
                with attempt:
                    atomic_write_text(path, data)
        except RetryError as exc:  # pragma: no cover - unlikely in tests
            self.metric_retry_exhausted.inc()
            raise exc.last_attempt.exception()

    def finalize(self, stats: RefactorStats) -> None:
        if self.config.fix_entrypoint:
            self._fix_entrypoints(stats)
        if self.config.report_csv and stats.fixes:
            write_report_csv(self.config.report_csv, stats.fixes)
        if self.config.report_json:
            write_report_json(self.config.report_json, self.context, stats)

    def _fix_entrypoints(self, stats: RefactorStats) -> None:
        assert self.config.fix_entrypoint is not None
        pattern = re.compile(r"(uvicorn(?:\.exe)?\s+)([\'\"]?)([A-Za-z_][\w\.]*:[A-Za-z_]\w*)(\2)")
        replacement = self.config.fix_entrypoint
        targets: List[Path] = [
            self.context.root / "run_application.bat",
            self.context.root / "Start-App.ps1",
            self.context.root / "Start_App.bat",
            self.context.root / "Start-App.sh",
            self.context.root / "README.md",
        ]
        for ext in ("*.ps1", "*.bat", "*.cmd", "*.sh", "*.py"):
            targets.extend(self.context.root.glob(ext))
        unique_targets: List[Path] = []
        seen: set[Path] = set()
        for file in targets:
            if file in seen:
                continue
            seen.add(file)
            unique_targets.append(file)
        for file in unique_targets:
            if not file.exists():
                continue
            content = file.read_text(encoding="utf-8")
            matches = list(pattern.finditer(content))
            if not matches:
                continue
            def _replace(match: re.Match[str]) -> str:
                prefix = match.group(1)
                quote = match.group(2) or ""
                suffix_quote = match.group(4) or ""
                return f"{prefix}{quote}{replacement}{suffix_quote}"

            new_content = pattern.sub(_replace, content)
            if self.config.apply_changes and new_content != content:
                self._write_with_retry(file, new_content)
            stats.add_fix(
                ModuleFix(
                    file_path=file,
                    action="entrypoint",
                    original=matches[0].group(0),
                    updated=f"uvicorn {replacement}",
                )
            )

    def metrics_payload(self) -> bytes:
        return generate_latest(self.registry)


# ---------------------------------------------------------------------------
# CLI entrypoints
# ---------------------------------------------------------------------------


def setup_logging(context: RepoContext) -> None:
    handler = StructuredLogHandler(context.correlation_id, context.namespace)
    LOGGER.handlers[:] = [handler]


def build_context(config: RefactorConfig) -> Tuple[RepoContext, List[Path]]:
    root = Path.cwd()
    ensure_agents_file(root)
    src = detect_src_path(root)
    namespace = build_namespace(root)
    files = iter_python_files(root)
    run_id = build_run_id(files, namespace)
    correlation_id = detect_correlation_id(config, run_id)
    context = RepoContext(
        root=root,
        src=src,
        namespace=namespace,
        run_id=run_id,
        correlation_id=correlation_id,
    )
    setup_logging(context)
    return context, files


def _serve_metrics_if_needed(config: RefactorConfig, refactorer: ImportRefactorer) -> None:
    if not config.serve_metrics:
        return
    if config.metrics_token is None:
        raise RefactorError("❌ توکن دسترسی به /metrics الزامی است.")

    from http.server import BaseHTTPRequestHandler, ThreadingHTTPServer

    class MetricsHandler(BaseHTTPRequestHandler):
        def do_GET(self) -> None:  # type: ignore[override]
            if self.path != f"/metrics?token={config.metrics_token}":
                self.send_response(403)
                self.end_headers()
                self.wfile.write("forbidden".encode("utf-8"))
                return
            payload = refactorer.metrics_payload()
            self.send_response(200)
            self.send_header("Content-Type", "text/plain; version=0.0.4")
            self.send_header("Content-Length", str(len(payload)))
            self.end_headers()
            self.wfile.write(payload)

        def log_message(self, format: str, *args: object) -> None:  # pragma: no cover - silence
            return

    server = ThreadingHTTPServer(("127.0.0.1", config.metrics_port), MetricsHandler)
    server.timeout = 0.1
    requests = max(1, config.metrics_requests)
    for _ in range(requests):
        server.handle_request()
    server.server_close()


def _execute(command: str, config: RefactorConfig) -> None:
    try:
        context, files = build_context(config)
    except RefactorError as exc:
        raise exc.to_exit()
    method = "GET" if command == "scan" else "POST"
    request = RequestEnvelope(
        rid=context.run_id,
        action=command,
        namespace=context.namespace,
        method=method,
    )
    store = IdempotencyStore(clock=config.clock)
    chain = MiddlewareChain(
        RateLimitMiddleware(config.rate_limit),
        IdempotencyMiddleware(store),
        AuthMiddleware(),
    )

    def handler(_: RequestEnvelope) -> None:
        refactorer = ImportRefactorer(context, config, store=store)
        stats = refactorer.process(files)
        refactorer.finalize(stats)
        _serve_metrics_if_needed(config, refactorer)
        if config.report_csv is None and config.report_json is None and not stats.fixes:
            LOGGER.info(
                "no-changes",
                extra={
                    "extra_data": {
                        "rid": context.run_id,
                        "action": command,
                        "status": "noop",
                    }
                },
            )

    try:
        chain.execute(request, handler)
    except RefactorError as exc:
        raise exc.to_exit()


@APP.command()
def scan(
    convert_relative: bool = typer.Option(False, help="Convert relative imports to absolute src.*"),
    report_csv: Optional[Path] = typer.Option(None, help="Path to CSV report."),
    report_json: Optional[Path] = typer.Option(None, help="Path to JSON report."),
    rate_limit: int = typer.Option(30, help="Maximum operations per run."),
    serve_metrics: bool = typer.Option(False, help="Expose metrics once."),
    metrics_port: int = typer.Option(9123, help="Metrics port."),
    metrics_token: Optional[str] = typer.Option(None, help="Access token for /metrics."),
) -> None:
    """Scan repository and report required import fixes."""
    config = RefactorConfig(
        convert_relative=convert_relative,
        apply_changes=False,
        report_csv=report_csv,
        report_json=report_json,
        rate_limit=rate_limit,
        serve_metrics=serve_metrics,
        metrics_port=metrics_port,
        metrics_token=metrics_token,
    )
    _execute("scan", config)


@APP.command()
def apply(
    convert_relative: bool = typer.Option(False, help="Convert relative imports to absolute src.*"),
    report_csv: Optional[Path] = typer.Option(None, help="Path to CSV report."),
    report_json: Optional[Path] = typer.Option(None, help="Path to JSON report."),
    fix_entrypoint: Optional[str] = typer.Option(None, help="Update uvicorn entrypoint."),
    rate_limit: int = typer.Option(30, help="Maximum operations per run."),
    serve_metrics: bool = typer.Option(False, help="Expose metrics once."),
    metrics_port: int = typer.Option(9123, help="Metrics port."),
    metrics_token: Optional[str] = typer.Option(None, help="Access token for /metrics."),
) -> None:
    """Apply import fixes to the repository."""
    if fix_entrypoint and not re.match(r"^[A-Za-z_][\w\.]*:[A-Za-z_]\w*$", fix_entrypoint):
        raise RefactorError("❌ مسیر uvicorn نامعتبر است.").to_exit()
    config = RefactorConfig(
        convert_relative=convert_relative,
        apply_changes=True,
        report_csv=report_csv,
        report_json=report_json,
        fix_entrypoint=fix_entrypoint,
        rate_limit=rate_limit,
        serve_metrics=serve_metrics,
        metrics_port=metrics_port,
        metrics_token=metrics_token,
    )
    _execute("apply", config)


if __name__ == "__main__":  # pragma: no cover
    APP()
